# Chapitre 98 - Patterns de conception avanc√©s en shell

> "Les patterns ne sont pas des recettes : ce sont des cristallisations d'exp√©rience, des formes √©mergentes de la sagesse collective des programmeurs qui ont affront√© les m√™mes d√©fis avant nous." - Christopher Alexander (adapt√© au shell)

## Introduction : L'architecture invisible du code

Imaginez-vous architecte d'un gratte-ciel : chaque pattern de conception est comme un √©l√©ment structural invisible qui donne √† l'ensemble sa solidit√© et son √©l√©gance. Les patterns de conception avanc√©s en shell ne sont pas de simples recettes - ce sont des architectures √©prouv√©es qui transforment le chaos du code complexe en harmonies maintenables.

Dans ce chapitre, nous explorerons les patterns architecturaux qui permettent d'√©crire du code shell non seulement fonctionnel, mais profond√©ment √©l√©gant et √©volutif. Ces patterns transcendent les langages pour r√©v√©ler les principes universels du logiciel bien con√ßu.

## Section 1 : Patterns structuraux fondamentaux

### 1.1 Pattern Builder : Construction d'objets complexes

Le Builder Pattern permet de construire des objets complexes √©tape par √©tape, isolant la logique de construction de la repr√©sentation finale :

```bash
#!/bin/bash

# Pattern Builder : Construction d'objets complexes
echo "=== Pattern Builder ==="

# Builder Pattern Implementation
Builder() {
    local self="$1"
    
    declare -A $self._build_parts
    declare -A $self._build_config
    
    # M√©thode de construction pour les scripts
    $self.build_script() {
        local script_type="$1"
        local output_file="$2"
        shift 2
        
        echo "Construction du script: $script_type -> $output_file"
        
        # Initialisation du builder
        $self._reset
        
        # Application des parties de construction
        case "$script_type" in
            monitoring)
                $self._build_monitoring_script "$@"
                ;;
            backup)
                $self._build_backup_script "$@"
                ;;
            deployment)
                $self._build_deployment_script "$@"
                ;;
            service)
                $self._build_service_script "$@"
                ;;
            *)
                echo "Type de script non support√©: $script_type" >&2
                return 1
                ;;
        esac
        
        # G√©n√©ration finale
        $self._generate_script "$output_file"
    }
    
    # Reset du builder
    $self._reset() {
        $self._build_parts=()
        $self._build_config=()
    }
    
    # Ajout d'une partie au script
    $self._add_part() {
        local part_name="$1"
        local part_content="$2"
        
        $self._build_parts["$part_name"]="${part_content}"
    }
    
    # Configuration du builder
    $self._set_config() {
        local key="$1"
        local value="$2"
        
        $self._build_config["$key"]="$value"
    }
    
    # Construction d'un script de monitoring
    $self._build_monitoring_script() {
        local target="$1"
        local metrics="${2:-cpu,memory,disk}"
        local interval="${3:-30}"
        
        $self._set_config "script_type" "monitoring"
        $self._set_config "target" "$target"
        $self._set_config "metrics" "$metrics"
        $self._set_config "interval" "$interval"
        
        # Partie header
        $self._add_part "header" "#!/bin/bash
# Script de monitoring g√©n√©r√© automatiquement
# Cible: $target
# M√©triques: $metrics
# Intervalle: ${interval}s
# G√©n√©r√© le: $(date)
"

        # Partie configuration
        $self._add_part "config" "
# Configuration
MONITOR_TARGET=\"$target\"
MONITOR_INTERVAL=\"$interval\"
MONITOR_METRICS=\"$metrics\"
DATA_FILE=\"/tmp/monitoring_\${MONITOR_TARGET}_\$(date +%Y%m%d).log\"
ALERT_THRESHOLD=\"80\"
"

        # Partie fonctions de collecte
        $self._add_part "functions" "
# Fonctions de collecte de m√©triques
collect_cpu() {
    top -bn1 | grep \"Cpu(s)\" | sed \"s/.*, *\\([0-9.]*\\)%* id.*/\\1/\" | awk '{print 100 - \$1}'
}

collect_memory() {
    free | grep Mem | awk '{printf \"%.1f\", \$3/\$2 * 100.0}'
}

collect_disk() {
    df / | tail -1 | awk '{print \$5}' | sed 's/%//'
}

collect_network() {
    local interface=\$(ip route | grep default | awk '{print \$5}' | head -1)
    if [[ -n \"\$interface\" ]]; then
        cat /proc/net/dev | grep \"\$interface\" | awk '{print \$2 \" \" \$10}'
    else
        echo \"0 0\"
    fi
}

# Fonction principale de collecte
collect_metrics() {
    local timestamp=\$(date +%s)
    local data_line=\"\$timestamp\"
    
    IFS=',' read -ra METRIC_ARRAY <<< \"\$MONITOR_METRICS\"
    for metric in \"\${METRIC_ARRAY[@]}\"; do
        case \"\$metric\" in
            cpu)
                local value=\$(collect_cpu)
                data_line=\"\$data_line,\${value:-0}\"
                ;;
            memory)
                local value=\$(collect_memory)
                data_line=\"\$data_line,\${value:-0}\"
                ;;
            disk)
                local value=\$(collect_disk)
                data_line=\"\$data_line,\${value:-0}\"
                ;;
            network)
                local values=\$(collect_network)
                data_line=\"\$data_line,\${values:-0 0}\"
                ;;
        esac
    done
    
    echo \"\$data_line\" >> \"\$DATA_FILE\"
    echo \"M√©triques collect√©es: \$timestamp\"
}

# Fonction d'alerte
check_alerts() {
    # Logique d'alerte simplifi√©e
    local latest_data=\$(tail -1 \"\$DATA_FILE\" 2>/dev/null)
    if [[ -n \"\$latest_data\" ]]; then
        local cpu_value=\$(echo \"\$latest_data\" | cut -d',' -f2)
        if (( \$(echo \"\$cpu_value > \$ALERT_THRESHOLD\" | bc -l 2>/dev/null || echo 0) )); then
            echo \"ALERTE: CPU > \$ALERT_THRESHOLD% (valeur: \$cpu_value%)\"
        fi
    fi
}
"

        # Partie main
        $self._add_part "main" "
# Point d'entr√©e principal
main() {
    echo \"D√©marrage du monitoring pour \$MONITOR_TARGET\"
    echo \"Intervalle: \$MONITOR_INTERVAL secondes\"
    echo \"M√©triques surveill√©es: \$MONITOR_METRICS\"
    echo \"Donn√©es: \$DATA_FILE\"
    echo
    
    # Cr√©ation du fichier de donn√©es
    echo \"timestamp,\$MONITOR_METRICS\" > \"\$DATA_FILE\"
    
    # Boucle de monitoring
    while true; do
        collect_metrics
        check_alerts
        sleep \"\$MONITOR_INTERVAL\"
    done
}

# Gestion des signaux
trap 'echo \"Arr√™t du monitoring...\"; exit 0' INT TERM

# Ex√©cution
if [[ \"\${BASH_SOURCE[0]}\" == \"\$0\" ]]; then
    main \"\$@\"
fi
"
    }
    
    # Construction d'un script de sauvegarde
    $self._build_backup_script() {
        local source_dir="$1"
        local dest_dir="$2"
        local compression="${3:-gzip}"
        local retention="${4:-7}"
        
        $self._set_config "script_type" "backup"
        $self._set_config "source" "$source_dir"
        $self._set_config "destination" "$dest_dir"
        $self._set_config "compression" "$compression"
        $self._set_config "retention" "$retention"
        
        $self._add_part "header" "#!/bin/bash
# Script de sauvegarde g√©n√©r√© automatiquement
# Source: $source_dir
# Destination: $dest_dir
# Compression: $compression
# R√©tention: ${retention} jours
# G√©n√©r√© le: $(date)
"

        $self._add_part "config" "
# Configuration
SOURCE_DIR=\"$source_dir\"
DEST_DIR=\"$dest_dir\"
COMPRESSION=\"$compression\"
RETENTION_DAYS=\"$retention\"
BACKUP_PREFIX=\"backup_\$(date +%Y%m%d_%H%M%S)\"
LOG_FILE=\"/var/log/backup.log\"
"

        $self._add_part "functions" "
# Fonctions de sauvegarde
create_backup() {
    local backup_name=\"\$1\"
    local source_path=\"\$2\"
    local dest_path=\"\$3\"
    
    echo \"Cr√©ation de la sauvegarde: \$backup_name\" | tee -a \"\$LOG_FILE\"
    
    case \"\$COMPRESSION\" in
        gzip)
            if tar -czf \"\$dest_path\" -C \"\$source_path\" . 2>/dev/null; then
                echo \"Sauvegarde gzip cr√©√©e: \$dest_path\" | tee -a \"\$LOG_FILE\"
                return 0
            fi
            ;;
        bzip2)
            if tar -cjf \"\$dest_path\" -C \"\$source_path\" . 2>/dev/null; then
                echo \"Sauvegarde bzip2 cr√©√©e: \$dest_path\" | tee -a \"\$LOG_FILE\"
                return 0
            fi
            ;;
        xz)
            if tar -cJf \"\$dest_path\" -C \"\$source_path\" . 2>/dev/null; then
                echo \"Sauvegarde xz cr√©√©e: \$dest_path\" | tee -a \"\$LOG_FILE\"
                return 0
            fi
            ;;
        none)
            if cp -r \"\$source_path\"/* \"\$dest_path\" 2>/dev/null; then
                echo \"Sauvegarde copie cr√©√©e: \$dest_path\" | tee -a \"\$LOG_FILE\"
                return 0
            fi
            ;;
    esac
    
    echo \"√âCHEC de cr√©ation de la sauvegarde\" | tee -a \"\$LOG_FILE\"
    return 1
}

cleanup_old_backups() {
    local backup_dir=\"\$1\"
    
    echo \"Nettoyage des anciennes sauvegardes...\" | tee -a \"\$LOG_FILE\"
    
    local old_backups=\$(find \"\$backup_dir\" -name \"backup_*.tar.*\" -mtime +\$RETENTION_DAYS 2>/dev/null)
    
    if [[ -n \"\$old_backups\" ]]; then
        echo \"\$old_backups\" | while read -r old_backup; do
            echo \"Suppression: \$old_backup\" | tee -a \"\$LOG_FILE\"
            rm -f \"\$old_backup\"
        done
    else
        echo \"Aucune ancienne sauvegarde √† supprimer\" | tee -a \"\$LOG_FILE\"
    fi
}

verify_backup() {
    local backup_file=\"\$1\"
    
    echo \"V√©rification de la sauvegarde: \$backup_file\" | tee -a \"\$LOG_FILE\"
    
    if [[ -f \"\$backup_file\" ]]; then
        local file_size=\$(stat -f%z \"\$backup_file\" 2>/dev/null || stat -c%s \"\$backup_file\" 2>/dev/null || echo \"0\")
        echo \"Taille: \$file_size octets\" | tee -a \"\$LOG_FILE\"
        
        # Test d'int√©grit√© basique
        if [[ \"\$COMPRESSION\" != \"none\" ]]; then
            if tar -tf \"\$backup_file\" >/dev/null 2>&1; then
                echo \"Archive intacte\" | tee -a \"\$LOG_FILE\"
                return 0
            else
                echo \"ERREUR: Archive corrompue\" | tee -a \"\$LOG_FILE\"
                return 1
            fi
        else
            echo \"V√©rification limit√©e pour sauvegarde non compress√©e\" | tee -a \"\$LOG_FILE\"
            return 0
        fi
    else
        echo \"ERREUR: Fichier de sauvegarde introuvable\" | tee -a \"\$LOG_FILE\"
        return 1
    fi
}
"

        $self._add_part "main" "
# Point d'entr√©e principal
main() {
    local timestamp=\$(date +%Y%m%d_%H%M%S)
    local backup_name=\"backup_\${timestamp}\"
    
    case \"\$COMPRESSION\" in
        none)
            local backup_file=\"\$DEST_DIR/\${backup_name}\"
            ;;
        *)
            local backup_file=\"\$DEST_DIR/\${backup_name}.tar.\${COMPRESSION}\"
            ;;
    esac
    
    echo \"=== SAUVEGARDE AUTOMATIS√âE ===\" | tee -a \"\$LOG_FILE\"
    echo \"Source: \$SOURCE_DIR\" | tee -a \"\$LOG_FILE\"
    echo \"Destination: \$backup_file\" | tee -a \"\$LOG_FILE\"
    echo \"D√©but: \$(date)\" | tee -a \"\$LOG_FILE\"
    echo | tee -a \"\$LOG_FILE\"
    
    # Cr√©ation du r√©pertoire de destination
    mkdir -p \"\$DEST_DIR\"
    
    # Ex√©cution de la sauvegarde
    if create_backup \"\$backup_name\" \"\$SOURCE_DIR\" \"\$backup_file\"; then
        if verify_backup \"\$backup_file\"; then
            echo \"‚úì Sauvegarde r√©ussie\" | tee -a \"\$LOG_FILE\"
            
            # Nettoyage
            cleanup_old_backups \"\$DEST_DIR\"
            
            echo \"Sauvegarde termin√©e avec succ√®s\" | tee -a \"\$LOG_FILE\"
            return 0
        fi
    fi
    
    echo \"‚ùå √âchec de la sauvegarde\" | tee -a \"\$LOG_FILE\"
    return 1
}

# Gestion des signaux
trap 'echo \"Interruption d√©tect√©e, annulation de la sauvegarde\" | tee -a \"\$LOG_FILE\"; exit 1' INT TERM

# Ex√©cution
if [[ \"\${BASH_SOURCE[0]}\" == \"\$0\" ]]; then
    main \"\$@\"
fi
"
    }
    
    # G√©n√©ration finale du script
    $self._generate_script() {
        local output_file="$1"
        
        {
            echo "${$self._build_parts[header]}"
            echo "${$self._build_parts[config]}"
            echo "${$self._build_parts[functions]}"
            echo "${$self._build_parts[main]}"
        } > "$output_file"
        
        chmod +x "$output_file"
        
        local script_type="${$self._build_config[script_type]}"
        echo "‚úì Script $script_type g√©n√©r√©: $output_file"
    }
}

# D√©monstration du Pattern Builder
echo "--- Pattern Builder ---"

Builder "script_builder"

echo
echo "--- Construction d'un script de monitoring ---"
script_builder.build_script "monitoring" "/tmp/monitoring_script.sh" "localhost" "cpu,memory,disk" "60"

echo
echo "Script de monitoring g√©n√©r√©:"
head -15 /tmp/monitoring_script.sh

echo
echo "--- Construction d'un script de sauvegarde ---"
script_builder.build_script "backup" "/tmp/backup_script.sh" "/home/user/documents" "/tmp/backups" "gzip" "30"

echo
echo "Script de sauvegarde g√©n√©r√©:"
head -15 /tmp/backup_script.sh

echo
echo "--- Test du script de monitoring ---"
timeout 3 bash /tmp/monitoring_script.sh &
sleep 2
kill %1 2>/dev/null || true

# Nettoyage
rm -f /tmp/monitoring_script.sh /tmp/backup_script.sh /tmp/monitoring_localhost_*.log
```

### 1.2 Pattern Factory : Cr√©ation d'objets sp√©cialis√©s

Le Factory Pattern encapsule la logique de cr√©ation d'objets, permettant de cr√©er des instances sp√©cialis√©es selon les besoins :

```bash
#!/bin/bash

# Pattern Factory : Cr√©ation d'objets sp√©cialis√©s
echo "=== Pattern Factory ==="

# Factory Pattern Implementation
Factory() {
    local self="$1"
    
    declare -A $self._registered_types
    declare -A $self._creation_strategies
    
    # Enregistrement d'un type cr√©able
    $self.register_type() {
        local type_name="$1"
        local creation_function="$2"
        local validation_function="${3:-}"
        
        $self._registered_types["$type_name"]="$creation_function"
        
        if [[ -n "$validation_function" ]]; then
            $self._registered_types["${type_name}_validator"]="$validation_function"
        fi
        
        echo "‚úì Type enregistr√©: $type_name"
    }
    
    # Cr√©ation d'une instance
    $self.create_instance() {
        local type_name="$1"
        shift
        
        local creation_func="${$self._registered_types[$type_name]}"
        
        if [[ -z "$creation_func" ]]; then
            echo "‚ùå Type non enregistr√©: $type_name" >&2
            return 1
        fi
        
        echo "Cr√©ation d'instance: $type_name"
        
        # Validation des param√®tres si un validateur existe
        local validator="${$self._registered_types[${type_name}_validator]}"
        if [[ -n "$validator" ]]; then
            if ! $validator "$@"; then
                echo "‚ùå Param√®tres invalides pour $type_name" >&2
                return 1
            fi
        fi
        
        # Ex√©cution de la fonction de cr√©ation
        $creation_func "$@"
    }
    
    # Factory Method sp√©cialis√© pour les connecteurs de base de donn√©es
    $self.create_database_connector() {
        local db_type="$1"
        shift
        
        case "$db_type" in
            mysql|mariadb)
                $self.create_instance "mysql_connector" "$@"
                ;;
            postgresql)
                $self.create_instance "postgres_connector" "$@"
                ;;
            sqlite)
                $self.create_instance "sqlite_connector" "$@"
                ;;
            mongodb)
                $self.create_instance "mongodb_connector" "$@"
                ;;
            *)
                echo "‚ùå Type de base de donn√©es non support√©: $db_type" >&2
                return 1
                ;;
        esac
    }
    
    # Factory pour les gestionnaires de cache
    $self.create_cache_manager() {
        local cache_type="$1"
        shift
        
        case "$cache_type" in
            redis)
                $self.create_instance "redis_cache" "$@"
                ;;
            memcached)
                $self.create_instance "memcached_cache" "$@"
                ;;
            filesystem)
                $self.create_instance "filesystem_cache" "$@"
                ;;
            memory)
                $self.create_instance "memory_cache" "$@"
                ;;
            *)
                echo "‚ùå Type de cache non support√©: $cache_type" >&2
                return 1
                ;;
        esac
    }
    
    # Factory pour les loggers
    $self.create_logger() {
        local log_type="$1"
        shift
        
        case "$log_type" in
            file)
                $self.create_instance "file_logger" "$@"
                ;;
            syslog)
                $self.create_instance "syslog_logger" "$@"
                ;;
            journald)
                $self.create_instance "journald_logger" "$@"
                ;;
            elasticsearch)
                $self.create_instance "elasticsearch_logger" "$@"
                ;;
            *)
                echo "‚ùå Type de logger non support√©: $log_type" >&2
                return 1
                ;;
        esac
    }
}

# Fonctions de cr√©ation pour les connecteurs de base de donn√©es
create_mysql_connector() {
    local host="$1"
    local port="${2:-3306}"
    local database="$3"
    local username="$4"
    local password="$5"
    
    cat << EOF
# Connecteur MySQL g√©n√©r√©
MYSQL_HOST="$host"
MYSQL_PORT="$port"
MYSQL_DATABASE="$database"
MYSQL_USERNAME="$username"
MYSQL_PASSWORD="$password"

mysql_connect() {
    mysql -h"\$MYSQL_HOST" -P"\$MYSQL_PORT" -u"\$MYSQL_USERNAME" -p"\$MYSQL_PASSWORD" "\$MYSQL_DATABASE"
}

mysql_query() {
    local query="\$1"
    mysql -h"\$MYSQL_HOST" -P"\$MYSQL_PORT" -u"\$MYSQL_USERNAME" -p"\$MYSQL_PASSWORD" -e"\$query" "\$MYSQL_DATABASE"
}

mysql_backup() {
    local output_file="\${1:-backup_\$(date +%Y%m%d_%H%M%S).sql}"
    mysqldump -h"\$MYSQL_HOST" -P"\$MYSQL_PORT" -u"\$MYSQL_USERNAME" -p"\$MYSQL_PASSWORD" "\$MYSQL_DATABASE" > "\$output_file"
    echo "Sauvegarde MySQL cr√©√©e: \$output_file"
}
EOF
}

validate_mysql_params() {
    local host="$1"
    local port="$2"
    local database="$3"
    local username="$4"
    local password="$5"
    
    [[ -n "$host" && -n "$database" && -n "$username" && -n "$password" ]] || return 1
    
    # Validation du port
    [[ "$port" =~ ^[0-9]+$ && "$port" -ge 1 && "$port" -le 65535 ]] || return 1
    
    return 0
}

create_postgres_connector() {
    local host="$1"
    local port="${2:-5432}"
    local database="$3"
    local username="$4"
    local password="$5"
    
    cat << EOF
# Connecteur PostgreSQL g√©n√©r√©
PGHOST="$host"
PGPORT="$port"
PGDATABASE="$database"
PGUSER="$username"
PGPASSWORD="$password"

postgres_connect() {
    psql -h"\$PGHOST" -p"\$PGPORT" -U"\$PGUSER" -d"\$PGDATABASE"
}

postgres_query() {
    local query="\$1"
    psql -h"\$PGHOST" -p"\$PGPORT" -U"\$PGUSER" -d"\$PGDATABASE" -c"\$query"
}

postgres_backup() {
    local output_file="\${1:-backup_\$(date +%Y%m%d_%H%M%S).sql}"
    pg_dump -h"\$PGHOST" -p"\$PGPORT" -U"\$PGUSER" -d"\$PGDATABASE" > "\$output_file"
    echo "Sauvegarde PostgreSQL cr√©√©e: \$output_file"
}
EOF
}

create_sqlite_connector() {
    local db_file="$1"
    
    cat << EOF
# Connecteur SQLite g√©n√©r√©
SQLITE_DB_FILE="$db_file"

sqlite_connect() {
    sqlite3 "\$SQLITE_DB_FILE"
}

sqlite_query() {
    local query="\$1"
    sqlite3 "\$SQLITE_DB_FILE" "\$query"
}

sqlite_backup() {
    local output_file="\${1:-\${SQLITE_DB_FILE}.backup}"
    sqlite3 "\$SQLITE_DB_FILE" ".backup \$output_file"
    echo "Sauvegarde SQLite cr√©√©e: \$output_file"
}
EOF
}

# Fonctions de cr√©ation pour les gestionnaires de cache
create_redis_cache() {
    local host="${1:-localhost}"
    local port="${2:-6379}"
    local password="$3"
    
    cat << EOF
# Cache Redis g√©n√©r√©
REDIS_HOST="$host"
REDIS_PORT="$port"
REDIS_PASSWORD="$password"

redis_set() {
    local key="\$1"
    local value="\$2"
    local ttl="\${3:-3600}"
    
    if [[ -n "\$REDIS_PASSWORD" ]]; then
        redis-cli -h "\$REDIS_HOST" -p "\$REDIS_PORT" -a "\$REDIS_PASSWORD" SET "\$key" "\$value" EX "\$ttl"
    else
        redis-cli -h "\$REDIS_HOST" -p "\$REDIS_PORT" SET "\$key" "\$value" EX "\$ttl"
    fi
}

redis_get() {
    local key="\$1"
    
    if [[ -n "\$REDIS_PASSWORD" ]]; then
        redis-cli -h "\$REDIS_HOST" -p "\$REDIS_PORT" -a "\$REDIS_PASSWORD" GET "\$key"
    else
        redis-cli -h "\$REDIS_HOST" -p "\$REDIS_PORT" GET "\$key"
    fi
}

redis_delete() {
    local key="\$1"
    
    if [[ -n "\$REDIS_PASSWORD" ]]; then
        redis-cli -h "\$REDIS_HOST" -p "\$REDIS_PORT" -a "\$REDIS_PASSWORD" DEL "\$key"
    else
        redis-cli -h "\$REDIS_HOST" -p "\$REDIS_PORT" DEL "\$key"
    fi
}
EOF
}

create_filesystem_cache() {
    local cache_dir="${1:-/tmp/cache}"
    
    cat << EOF
# Cache syst√®me de fichiers g√©n√©r√©
CACHE_DIR="$cache_dir"

fs_cache_set() {
    local key="\$1"
    local value="\$2"
    local ttl="\${3:-3600}"
    
    mkdir -p "\$CACHE_DIR"
    local cache_file="\$CACHE_DIR/\$(echo "\$key" | md5sum | cut -d' ' -f1)"
    
    # Stockage avec timestamp d'expiration
    local expiry=\$(( \$(date +%s) + ttl ))
    echo "\$expiry" > "\$cache_file.expiry"
    echo "\$value" > "\$cache_file"
}

fs_cache_get() {
    local key="\$1"
    
    local cache_file="\$CACHE_DIR/\$(echo "\$key" | md5sum | cut -d' ' -f1)"
    
    if [[ -f "\$cache_file" && -f "\$cache_file.expiry" ]]; then
        local expiry=\$(cat "\$cache_file.expiry")
        local now=\$(date +%s)
        
        if (( now < expiry )); then
            cat "\$cache_file"
            return 0
        else
            # Cache expir√©, nettoyage
            rm -f "\$cache_file" "\$cache_file.expiry"
        fi
    fi
    
    return 1
}

fs_cache_delete() {
    local key="\$1"
    
    local cache_file="\$CACHE_DIR/\$(echo "\$key" | md5sum | cut -d' ' -f1)"
    rm -f "\$cache_file" "\$cache_file.expiry"
}
EOF
}

# Fonctions de cr√©ation pour les loggers
create_file_logger() {
    local log_file="$1"
    local max_size="${2:-10485760}"  # 10MB par d√©faut
    local max_files="${3:-5}"
    
    cat << EOF
# Logger fichier g√©n√©r√©
LOG_FILE="$log_file"
LOG_MAX_SIZE="$max_size"
LOG_MAX_FILES="$max_files"

file_log() {
    local level="\$1"
    local message="\$2"
    local timestamp=\$(date '+%Y-%m-%d %H:%M:%S')
    
    # Rotation si n√©cessaire
    if [[ -f "\$LOG_FILE" ]]; then
        local size=\$(stat -f%z "\$LOG_FILE" 2>/dev/null || stat -c%s "\$LOG_FILE" 2>/dev/null || echo "0")
        if (( size > LOG_MAX_SIZE )); then
            rotate_log_file
        fi
    fi
    
    echo "[\$timestamp] [\$level] \$message" >> "\$LOG_FILE"
}

rotate_log_file() {
    # Rotation des anciens fichiers
    for ((i=LOG_MAX_FILES; i>=1; i--)); do
        if [[ -f "\${LOG_FILE}.\$i" ]]; then
            if (( i == LOG_MAX_FILES )); then
                rm -f "\${LOG_FILE}.\$i"
            else
                mv "\${LOG_FILE}.\$i" "\${LOG_FILE}.\$((i+1))"
            fi
        fi
    done
    
    # Renommage du fichier actuel
    if [[ -f "\$LOG_FILE" ]]; then
        mv "\$LOG_FILE" "\${LOG_FILE}.1"
    fi
}
EOF
}

create_syslog_logger() {
    local facility="${1:-user}"
    local priority="${2:-info}"
    
    cat << EOF
# Logger syslog g√©n√©r√©
SYSLOG_FACILITY="$facility"
SYSLOG_PRIORITY="$priority"

syslog_log() {
    local level="\$1"
    local message="\$2"
    
    # Mapping des niveaux aux priorit√©s syslog
    case "\$level" in
        DEBUG) prio="debug" ;;
        INFO) prio="info" ;;
        WARN|WARNING) prio="warning" ;;
        ERROR) prio="err" ;;
        CRITICAL|FATAL) prio="crit" ;;
        *) prio="info" ;;
    esac
    
    logger -p "\${SYSLOG_FACILITY}.\$prio" "\$message"
}
EOF
}

# D√©monstration du Pattern Factory
echo "--- Pattern Factory ---"

Factory "object_factory"

# Enregistrement des types
echo "Enregistrement des types de connecteurs de base de donn√©es..."
object_factory.register_type "mysql_connector" "create_mysql_connector" "validate_mysql_params"
object_factory.register_type "postgres_connector" "create_postgres_connector"
object_factory.register_type "sqlite_connector" "create_sqlite_connector"

echo
echo "Enregistrement des types de cache..."
object_factory.register_type "redis_cache" "create_redis_cache"
object_factory.register_type "filesystem_cache" "create_filesystem_cache"

echo
echo "Enregistrement des types de loggers..."
object_factory.register_type "file_logger" "create_file_logger"
object_factory.register_type "syslog_logger" "create_syslog_logger"

echo
echo "--- Cr√©ation d'un connecteur MySQL ---"
object_factory.create_database_connector "mysql" "localhost" "3306" "mydb" "user" "password" > /tmp/mysql_connector.sh

echo "Connecteur MySQL cr√©√©:"
head -10 /tmp/mysql_connector.sh

echo
echo "--- Cr√©ation d'un cache Redis ---"
object_factory.create_cache_manager "redis" "localhost" "6379" "" > /tmp/redis_cache.sh

echo "Cache Redis cr√©√©:"
head -10 /tmp/redis_cache.sh

echo
echo "--- Cr√©ation d'un logger fichier ---"
object_factory.create_logger "file" "/var/log/myapp.log" "10485760" "10" > /tmp/file_logger.sh

echo "Logger fichier cr√©√©:"
head -10 /tmp/file_logger.sh

echo
echo "--- Cr√©ation d'un logger syslog ---"
object_factory.create_logger "syslog" "local0" "info" > /tmp/syslog_logger.sh

echo "Logger syslog cr√©√©:"
head -10 /tmp/syslog_logger.sh

# Nettoyage
rm -f /tmp/mysql_connector.sh /tmp/redis_cache.sh /tmp/file_logger.sh /tmp/syslog_logger.sh
```

## Section 2 : Patterns comportementaux avanc√©s

### 2.1 Pattern Strategy : Algorithmes interchangeables

Le Strategy Pattern permet de s√©lectionner dynamiquement des algorithmes ou strat√©gies d'ex√©cution :

```bash
#!/bin/bash

# Pattern Strategy : Algorithmes interchangeables
echo "=== Pattern Strategy ==="

# Strategy Pattern Implementation
StrategyContext() {
    local self="$1"
    
    declare -A $self._strategies
    declare -A $self._current_strategy
    
    # Enregistrement d'une strat√©gie
    $self.register_strategy() {
        local strategy_name="$1"
        local strategy_function="$2"
        local description="$3"
        
        $self._strategies["${strategy_name}_function"]="$strategy_function"
        $self._strategies["${strategy_name}_description"]="$description"
        
        echo "‚úì Strat√©gie enregistr√©e: $strategy_name"
    }
    
    # S√©lection d'une strat√©gie
    $self.set_strategy() {
        local strategy_name="$1"
        
        if [[ -z "${$self._strategies[${strategy_name}_function]}" ]]; then
            echo "‚ùå Strat√©gie inconnue: $strategy_name" >&2
            return 1
        fi
        
        $self._current_strategy["name"]="$strategy_name"
        $self._current_strategy["function"]="${$self._strategies[${strategy_name}_function]}"
        
        echo "‚úì Strat√©gie s√©lectionn√©e: $strategy_name"
    }
    
    # Ex√©cution avec la strat√©gie courante
    $self.execute_strategy() {
        local current_func="${$self._current_strategy[function]}"
        
        if [[ -z "$current_func" ]]; then
            echo "‚ùå Aucune strat√©gie s√©lectionn√©e" >&2
            return 1
        fi
        
        echo "Ex√©cution avec strat√©gie: ${$self._current_strategy[name]}"
        $current_func "$@"
    }
    
    # Ex√©cution avec s√©lection automatique de strat√©gie
    $self.execute_with_auto_selection() {
        shift
        local -a args=("$@")
        
        # Analyse des arguments pour s√©lection automatique
        local strategy_name=""
        
        # S√©lection bas√©e sur la taille des donn√©es
        if [[ ${#args[@]} -gt 10 ]]; then
            strategy_name="parallel_processing"
        elif [[ -n "${args[0]}" && "${args[0]}" =~ ^[0-9]+$ && "${args[0]}" -gt 1000 ]]; then
            strategy_name="optimized_sorting"
        elif [[ -n "${args[0]}" && -f "${args[0]}" ]]; then
            local file_size
            file_size="$(stat -f%z "${args[0]}" 2>/dev/null || stat -c%s "${args[0]}" 2>/dev/null || echo "0")"
            if (( file_size > 1048576 )); then  # > 1MB
                strategy_name="memory_efficient"
            else
                strategy_name="standard_processing"
            fi
        else
            strategy_name="standard_processing"
        fi
        
        $self.set_strategy "$strategy_name"
        $self.execute_strategy "${args[@]}"
    }
    
    # Liste des strat√©gies disponibles
    $self.list_strategies() {
        echo "Strat√©gies disponibles:"
        
        for strategy_key in "${!$self._strategies[@]}"; do
            if [[ "$strategy_key" =~ _description$ ]]; then
                local strategy_name="${strategy_key%_description}"
                local description="${$self._strategies[$strategy_key]}"
                
                echo "  $strategy_name: $description"
            fi
        done
    }
}

# Strat√©gies de traitement de donn√©es
data_processing_strategy_standard() {
    echo "=== STRAT√âGIE STANDARD ==="
    
    local -a data=("$@")
    
    echo "Traitement s√©quentiel des ${#data[@]} √©l√©ments..."
    
    for item in "${data[@]}"; do
        # Traitement simul√©
        echo "  Traitement: $item -> ${item}_processed"
        sleep 0.1
    done
    
    echo "‚úì Traitement standard termin√©"
}

data_processing_strategy_parallel() {
    echo "=== STRAT√âGIE PARALL√àLE ==="
    
    local -a data=("$@")
    
    echo "Traitement parall√®le des ${#data[@]} √©l√©ments..."
    
    # Traitement parall√®le simul√©
    local pids=()
    
    for item in "${data[@]}"; do
        (
            echo "  Traitement parall√®le: $item -> ${item}_parallel_processed"
            sleep 0.2
        ) &
        pids+=($!)
    done
    
    # Attente de tous les processus
    for pid in "${pids[@]}"; do
        wait "$pid"
    done
    
    echo "‚úì Traitement parall√®le termin√©"
}

data_processing_strategy_memory_efficient() {
    echo "=== STRAT√âGIE M√âMOIRE EFFICACE ==="
    
    local file_path="$1"
    
    echo "Traitement en flux du fichier: $file_path"
    
    if [[ ! -f "$file_path" ]]; then
        echo "‚ùå Fichier introuvable: $file_path"
        return 1
    fi
    
    local line_count=0
    while IFS= read -r line; do
        # Traitement ligne par ligne pour √©conomiser la m√©moire
        echo "  Ligne $((++line_count)): ${line:0:50}... -> processed"
    done < "$file_path"
    
    echo "‚úì Traitement en flux termin√© ($line_count lignes)"
}

data_processing_strategy_optimized_sorting() {
    echo "=== STRAT√âGIE TRI OPTIMIS√â ==="
    
    local array_size="$1"
    
    echo "Tri optimis√© pour tableau de taille $array_size"
    
    if (( array_size > 1000 )); then
        echo "Utilisation de l'algorithme de tri externe..."
        # Simulation d'un tri externe
        echo "  Phase 1: Division en chunks"
        echo "  Phase 2: Tri des chunks"
        echo "  Phase 3: Fusion des chunks tri√©s"
    else
        echo "Utilisation du tri rapide standard..."
        # Simulation d'un tri rapide
        echo "  Partitionnement r√©cursif"
        echo "  Tri des partitions"
    fi
    
    echo "‚úì Tri optimis√© termin√©"
}

# Strat√©gies de sauvegarde
backup_strategy_full() {
    echo "=== STRAT√âGIE SAUVEGARDE COMPL√àTE ==="
    
    local source="$1"
    local destination="$2"
    
    echo "Sauvegarde compl√®te: $source -> $destination"
    
    # Simulation de sauvegarde compl√®te
    mkdir -p "$destination"
    echo "Copie compl√®te de tous les fichiers..."
    echo "‚úì Sauvegarde compl√®te termin√©e"
}

backup_strategy_incremental() {
    echo "=== STRAT√âGIE SAUVEGARDE INCR√âMENTALE ==="
    
    local source="$1"
    local destination="$2"
    
    echo "Sauvegarde incr√©mentale: $source -> $destination"
    
    # Simulation de sauvegarde incr√©mentale
    echo "Analyse des fichiers modifi√©s depuis la derni√®re sauvegarde..."
    echo "Sauvegarde des fichiers modifi√©s uniquement..."
    echo "‚úì Sauvegarde incr√©mentale termin√©e"
}

backup_strategy_differential() {
    echo "=== STRAT√âGIE SAUVEGARDE DIFF√âRENTIELLE ==="
    
    local source="$1"
    local destination="$2"
    
    echo "Sauvegarde diff√©rentielle: $source -> $destination"
    
    # Simulation de sauvegarde diff√©rentielle
    echo "Comparaison avec la derni√®re sauvegarde compl√®te..."
    echo "Sauvegarde des fichiers modifi√©s..."
    echo "‚úì Sauvegarde diff√©rentielle termin√©e"
}

# Strat√©gies de d√©ploiement
deployment_strategy_immediate() {
    echo "=== STRAT√âGIE D√âPLOIEMENT IMM√âDIAT ==="
    
    local app_name="$1"
    local version="$2"
    
    echo "D√©ploiement imm√©diat de $app_name v$version"
    
    # Simulation de d√©ploiement imm√©diat
    echo "Arr√™t de l'ancienne version..."
    echo "D√©ploiement de la nouvelle version..."
    echo "D√©marrage de la nouvelle version..."
    echo "‚úì D√©ploiement imm√©diat termin√©"
}

deployment_strategy_blue_green() {
    echo "=== STRAT√âGIE D√âPLOIEMENT BLUE/GREEN ==="
    
    local app_name="$1"
    local version="$2"
    
    echo "D√©ploiement blue/green de $app_name v$version"
    
    # Simulation de d√©ploiement blue/green
    echo "D√©ploiement vers l'environnement green..."
    echo "Tests de sant√© de l'environnement green..."
    echo "Basculement du trafic vers green..."
    echo "‚úì D√©ploiement blue/green termin√©"
}

deployment_strategy_canary() {
    echo "=== STRAT√âGIE D√âPLOIEMENT CANARY ==="
    
    local app_name="$1"
    local version="$2"
    local percentage="${3:-10}"
    
    echo "D√©ploiement canary de $app_name v$version ($percentage%)"
    
    # Simulation de d√©ploiement canary
    echo "D√©ploiement initial √† $percentage% du trafic..."
    echo "Surveillance des m√©triques..."
    echo "Augmentation progressive du trafic..."
    echo "‚úì D√©ploiement canary termin√©"
}

# D√©monstration du Pattern Strategy
echo "--- Pattern Strategy ---"

StrategyContext "strategy_context"

# Enregistrement des strat√©gies de traitement de donn√©es
strategy_context.register_strategy "standard_processing" "data_processing_strategy_standard" "Traitement s√©quentiel standard"
strategy_context.register_strategy "parallel_processing" "data_processing_strategy_parallel" "Traitement parall√®le pour gros volumes"
strategy_context.register_strategy "memory_efficient" "data_processing_strategy_memory_efficient" "Traitement en flux pour √©conomiser la m√©moire"
strategy_context.register_strategy "optimized_sorting" "data_processing_strategy_optimized_sorting" "Tri optimis√© selon la taille des donn√©es"

# Enregistrement des strat√©gies de sauvegarde
strategy_context.register_strategy "full_backup" "backup_strategy_full" "Sauvegarde compl√®te de tous les fichiers"
strategy_context.register_strategy "incremental_backup" "backup_strategy_incremental" "Sauvegarde des fichiers modifi√©s uniquement"
strategy_context.register_strategy "differential_backup" "backup_strategy_differential" "Sauvegarde diff√©rentielle depuis la derni√®re compl√®te"

# Enregistrement des strat√©gies de d√©ploiement
strategy_context.register_strategy "immediate_deployment" "deployment_strategy_immediate" "D√©ploiement imm√©diat avec interruption"
strategy_context.register_strategy "blue_green_deployment" "deployment_strategy_blue_green" "D√©ploiement sans interruption"
strategy_context.register_strategy "canary_deployment" "deployment_strategy_canary" "D√©ploiement progressif avec tests"

echo
echo "--- Liste des strat√©gies ---"
strategy_context.list_strategies

echo
echo "--- Ex√©cution avec s√©lection manuelle ---"

echo "1. Traitement de donn√©es standard:"
strategy_context.set_strategy "standard_processing"
strategy_context.execute_strategy "data1" "data2" "data3"

echo
echo "2. Sauvegarde compl√®te:"
strategy_context.set_strategy "full_backup"
strategy_context.execute_strategy "/home/user" "/backup/full"

echo
echo "3. D√©ploiement blue/green:"
strategy_context.set_strategy "blue_green_deployment"
strategy_context.execute_strategy "myapp" "2.1.0"

echo
echo "--- Ex√©cution avec s√©lection automatique ---"

echo "4. Traitement avec s√©lection automatique (petite liste):"
strategy_context.execute_with_auto_selection "data1" "data2"

echo
echo "5. Traitement avec s√©lection automatique (grande liste):"
# Cr√©ation d'une grande liste pour d√©clencher la strat√©gie parall√®le
large_data=()
for i in {1..15}; do
    large_data+=("data$i")
done
strategy_context.execute_with_auto_selection "${large_data[@]}"

echo
echo "6. Traitement de fichier volumineux:"
echo "test content line 1" > /tmp/large_test.txt
for i in {2..100}; do
    echo "test content line $i with more data to make it larger" >> /tmp/large_test.txt
done
strategy_context.execute_with_auto_selection "/tmp/large_test.txt"

# Nettoyage
rm -f /tmp/large_test.txt
```

### 2.2 Pattern Observer : Notification d'√©v√©nements

Le Observer Pattern permet √† des objets de s'abonner √† des √©v√©nements et d'√™tre notifi√©s automatiquement :

```bash
#!/bin/bash

# Pattern Observer : Notification d'√©v√©nements
echo "=== Pattern Observer ==="

# Observer Pattern Implementation
Observable() {
    local self="$1"
    
    declare -A $self._observers
    declare -A $self._events
    
    # Ajout d'un observateur
    $self.add_observer() {
        local event_type="$1"
        local observer_id="$2"
        local callback_function="$3"
        
        local observer_key="${event_type}_${observer_id}"
        $self._observers["$observer_key"]="$callback_function"
        
        echo "‚úì Observateur ajout√©: $observer_id pour $event_type"
    }
    
    # Suppression d'un observateur
    $self.remove_observer() {
        local event_type="$1"
        local observer_id="$2"
        
        local observer_key="${event_type}_${observer_id}"
        unset $self._observers["$observer_key"]
        
        echo "‚úì Observateur supprim√©: $observer_id pour $event_type"
    }
    
    # Notification des observateurs
    $self.notify_observers() {
        local event_type="$1"
        shift
        local -a event_data=("$@")
        
        echo "üì¢ Notification √©v√©nement: $event_type"
        
        local observer_count=0
        
        for observer_key in "${!$self._observers[@]}"; do
            if [[ "$observer_key" =~ ^${event_type}_ ]]; then
                local callback="${$self._observers[$observer_key]}"
                local observer_id="${observer_key#${event_type}_}"
                
                echo "  Notification de $observer_id"
                
                # Ex√©cution du callback en arri√®re-plan
                (
                    eval "$callback" "${event_data[@]}"
                ) &
                
                ((observer_count++))
            fi
        done
        
        if (( observer_count == 0 )); then
            echo "  Aucun observateur pour cet √©v√©nement"
        else
            echo "  $observer_count observateur(s) notifi√©(s)"
        fi
        
        # Historique des √©v√©nements
        local event_record="$(date +%s):$event_type:${event_data[*]}"
        $self._events["$(date +%s_%N)"]="$event_record"
    }
    
    # √âmission d'un √©v√©nement personnalis√©
    $self.emit_event() {
        local event_type="$1"
        shift
        
        $self.notify_observers "$event_type" "$@"
    }
    
    # Liste des observateurs
    $self.list_observers() {
        local event_filter="${1:-}"
        
        echo "Observateurs enregistr√©s:"
        
        for observer_key in "${!$self._observers[@]}"; do
            if [[ -z "$event_filter" || "$observer_key" =~ ^${event_filter}_ ]]; then
                local event_type="${observer_key%%_*}"
                local observer_id="${observer_key#*_}"
                local callback="${$self._observers[$observer_key]}"
                
                echo "  $event_type -> $observer_id (${callback:0:30}...)"
            fi
        done
    }
    
    # Statistiques des √©v√©nements
    $self.get_event_stats() {
        echo "Statistiques des √©v√©nements:"
        
        local total_events="${#$self._events[@]}"
        local -A event_types
        
        for event_key in "${!$self._events[@]}"; do
            local event_record="${$self._events[$event_key]}"
            local event_type="$(echo "$event_record" | cut -d: -f2)"
            ((event_types["$event_type"]++))
        done
        
        echo "  Total √©v√©nements: $total_events"
        
        for event_type in "${!event_types[@]}"; do
            echo "  $event_type: ${event_types[$event_type]}"
        done
    }
}

# Exemple d'observables sp√©cialis√©s
SystemMonitor() {
    local self="$1"
    
    # H√©ritage de l'observable de base
    Observable "$self"
    
    # M√©thodes sp√©cialis√©es pour la surveillance syst√®me
    $self.monitor_cpu() {
        local threshold="${1:-80}"
        
        while true; do
            local cpu_usage
            cpu_usage="$(top -bn1 | grep "Cpu(s)" | sed "s/.*, *\([0-9.]*\)%* id.*/\1/" | awk '{print 100 - $1}')"
            
            if (( $(echo "$cpu_usage > $threshold" | bc -l) )); then
                $self.emit_event "cpu_high" "$cpu_usage" "$threshold"
            fi
            
            sleep 5
        done
    }
    
    $self.monitor_memory() {
        local threshold="${1:-85}"
        
        while true; do
            local mem_usage
            mem_usage="$(free | grep Mem | awk '{printf "%.1f", $3/$2 * 100.0}')"
            
            if (( $(echo "$mem_usage > $threshold" | bc -l) )); then
                $self.emit_event "memory_high" "$mem_usage" "$threshold"
            fi
            
            sleep 10
        done
    }
    
    $self.monitor_disk() {
        local threshold="${1:-90}"
        local mount_point="${2:-/}"
        
        while true; do
            local disk_usage
            disk_usage="$(df "$mount_point" | tail -1 | awk '{print $5}' | sed 's/%//')"
            
            if (( disk_usage > threshold )); then
                $self.emit_event "disk_high" "$disk_usage" "$threshold" "$mount_point"
            fi
            
            sleep 60
        done
    }
}

FileWatcher() {
    local self="$1"
    
    Observable "$self"
    
    declare -A $self._watched_files
    
    $self.watch_file() {
        local file_path="$1"
        
        if [[ ! -f "$file_path" ]]; then
            echo "‚ùå Fichier introuvable: $file_path"
            return 1
        fi
        
        local initial_mtime
        initial_mtime="$(stat -c %Y "$file_path" 2>/dev/null || stat -f %m "$file_path")"
        $self._watched_files["$file_path"]="$initial_mtime"
        
        echo "‚úì Surveillance d√©marr√©e: $file_path"
        
        while true; do
            if [[ ! -f "$file_path" ]]; then
                $self.emit_event "file_deleted" "$file_path"
                unset $self._watched_files["$file_path"]
                break
            fi
            
            local current_mtime
            current_mtime="$(stat -c %Y "$file_path" 2>/dev/null || stat -f %m "$file_path")"
            local last_mtime="${$self._watched_files[$file_path]}"
            
            if [[ "$current_mtime" != "$last_mtime" ]]; then
                local file_size
                file_size="$(stat -c %s "$file_path" 2>/dev/null || stat -f %z "$file_path")"
                
                $self.emit_event "file_modified" "$file_path" "$file_size" "$current_mtime"
                $self._watched_files["$file_path"]="$current_mtime"
            fi
            
            sleep 2
        done
    }
    
    $self.watch_directory() {
        local dir_path="$1"
        
        if [[ ! -d "$dir_path" ]]; then
            echo "‚ùå R√©pertoire introuvable: $dir_path"
            return 1
        fi
        
        echo "‚úì Surveillance d√©marr√©e: $dir_path"
        
        # Obtention de l'√©tat initial
        local initial_state
        initial_state="$(find "$dir_path" -type f -exec stat -c '%Y %n' {} \; 2>/dev/null | sort)"
        
        while true; do
            local current_state
            current_state="$(find "$dir_path" -type f -exec stat -c '%Y %n' {} \; 2>/dev/null | sort)"
            
            if [[ "$current_state" != "$initial_state" ]]; then
                $self.emit_event "directory_changed" "$dir_path"
                initial_state="$current_state"
            fi
            
            sleep 5
        done
    }
}

# Fonctions callback pour les observateurs
cpu_alert_handler() {
    local cpu_usage="$1"
    local threshold="$2"
    
    echo "üö® ALERTE CPU: $cpu_usage% (seuil: $threshold%)"
    echo "Recommandations: V√©rifier les processus consommateurs, envisager red√©marrage"
}

memory_alert_handler() {
    local mem_usage="$1"
    local threshold="$2"
    
    echo "üö® ALERTE M√âMOIRE: $mem_usage% (seuil: $threshold%)"
    echo "Recommandations: Lib√©rer la m√©moire, v√©rifier les fuites"
}

disk_alert_handler() {
    local disk_usage="$1"
    local threshold="$2"
    local mount_point="$3"
    
    echo "üö® ALERTE DISQUE: $disk_usage% utilis√© sur $mount_point (seuil: $threshold%)"
    echo "Recommandations: Nettoyer les fichiers temporaires, ajouter de l'espace"
}

file_modified_handler() {
    local file_path="$1"
    local file_size="$2"
    local mtime="$3"
    
    echo "üìù FICHIER MODIFI√â: $file_path"
    echo "  Taille: $file_size octets"
    echo "  Modifi√©: $(date -d "@$mtime" '+%Y-%m-%d %H:%M:%S')"
}

log_error_handler() {
    local log_line="$1"
    
    if echo "$log_line" | grep -qi "error\|exception\|fail"; then
        echo "üö® ERREUR D√âTECT√âE dans les logs:"
        echo "  $log_line"
    fi
}

# D√©monstration du Pattern Observer
echo "--- Pattern Observer ---"

# Cr√©ation des observables
SystemMonitor "system_monitor"
FileWatcher "file_watcher"

# Enregistrement des observateurs pour le monitoring syst√®me
system_monitor.add_observer "cpu_high" "cpu_alert" "cpu_alert_handler"
system_monitor.add_observer "memory_high" "memory_alert" "memory_alert_handler"
system_monitor.add_observer "disk_high" "disk_alert" "disk_alert_handler"

# Enregistrement des observateurs pour la surveillance de fichiers
file_watcher.add_observer "file_modified" "file_logger" "file_modified_handler"

echo
echo "--- Liste des observateurs ---"
system_monitor.list_observers
file_watcher.list_observers

echo
echo "--- Test des √©v√©nements (simulation) ---"

echo "1. Simulation d'√©v√©nement CPU √©lev√©:"
system_monitor.emit_event "cpu_high" "85.5" "80"

echo
echo "2. Simulation d'√©v√©nement m√©moire √©lev√©:"
system_monitor.emit_event "memory_high" "90.2" "85"

echo
echo "3. Simulation d'√©v√©nement disque plein:"
system_monitor.emit_event "disk_high" "95" "90" "/"

echo
echo "4. Test de surveillance de fichier:"
echo "Cr√©ation d'un fichier de test..."
echo "Contenu initial" > /tmp/test_file.txt

# Lancement de la surveillance en arri√®re-plan
file_watcher.watch_file "/tmp/test_file.txt" &
watcher_pid=$!

sleep 1

echo "Modification du fichier..."
echo "Contenu modifi√© √† $(date)" >> /tmp/test_file.txt

sleep 3

# Arr√™t de la surveillance
kill $watcher_pid 2>/dev/null || true

echo
echo "--- Statistiques des √©v√©nements ---"
system_monitor.get_event_stats
file_watcher.get_event_stats

# Nettoyage
rm -f /tmp/test_file.txt
```

## Conclusion : L'architecture comme langage universel

Les patterns de conception avanc√©s en shell transcendent les simples recettes de programmation pour devenir un langage architectural universel. Ils permettent d'exprimer des concepts complexes - construction d'objets, strat√©gies interchangeables, notification d'√©v√©nements - dans un paradigme qui respecte les contraintes et exploite les forces du shell.

**Points cl√©s √† retenir :**

1. **Builder Pattern** : Construction √©tape par √©tape d'objets complexes avec s√©paration des responsabilit√©s
2. **Factory Pattern** : Cr√©ation d'objets sp√©cialis√©s avec encapsulation de la logique d'instanciation
3. **Strategy Pattern** : S√©lection dynamique d'algorithmes avec s√©lection automatique bas√©e sur le contexte
4. **Observer Pattern** : Notification d'√©v√©nements avec d√©couplage entre producteurs et consommateurs

Dans le prochain chapitre, nous explorerons les patterns architecturaux pour syst√®mes complexes, o√π ces patterns se combinent pour former des architectures compl√®tes capables de g√©rer les d√©fis les plus sophistiqu√©s de l'automatisation moderne.

---

**Exercice pratique :** Cr√©ez un syst√®me complet utilisant tous les patterns pr√©sent√©s :
- Builder pour g√©n√©rer des pipelines CI/CD complexes
- Factory pour cr√©er diff√©rents types de connecteurs de services
- Strategy pour impl√©menter diff√©rentes politiques de d√©ploiement
- Observer pour surveiller et r√©agir aux √©v√©nements syst√®me

**R√©flexion :** Comment ces patterns peuvent-ils √™tre combin√©s pour cr√©er des frameworks d'automatisation auto-adaptatifs capables d'√©voluer en fonction de leur environnement et de leurs besoins ?

